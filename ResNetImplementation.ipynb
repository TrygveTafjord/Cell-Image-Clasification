{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuwVgG3Vbbka"
      },
      "source": [
        "# Artificial Neural Networks and Deep Learning\n",
        "\n",
        "---\n",
        "\n",
        "## Homework 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw_-hFm6bjY6"
      },
      "source": [
        "## üåê Connect Colab to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2S4GWr3Uoa8",
        "outputId": "f53bb45b-492d-4fad-f6f5-97447465ac28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive not mounted, so nothing to flush and unmount.\n",
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/[2024-2025] AN2DL/Homework 1\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "\n",
        "drive.mount('/gdrive', force_remount=True)\n",
        "%cd /gdrive/My Drive/[2024-2025] AN2DL/Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7IqZP5Iblna"
      },
      "source": [
        "## ‚öôÔ∏è Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-cv --upgrade"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbmzNMN9rNSC",
        "outputId": "9e7dd39a-bfe4-4829-d596-e1d80c9608d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-cv\n",
            "  Downloading keras_cv-0.9.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-cv) (25.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras-cv) (1.4.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from keras-cv) (2024.11.6)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.11/dist-packages (from keras-cv) (4.9.9)\n",
            "Collecting keras-core (from keras-cv)\n",
            "  Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from keras-cv) (0.3.12)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub->keras-cv) (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (3.14.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from keras-core->keras-cv) (0.1.9)\n",
            "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.7.2)\n",
            "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (1.13.0)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (4.2.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (18.1.0)\n",
            "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (1.17.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (3.1.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (0.10.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow-datasets->keras-cv) (1.17.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (0.8.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (6.5.2)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (4.14.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow-datasets->keras-cv) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub->keras-cv) (2025.8.3)\n",
            "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->keras-core->keras-cv) (25.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow-datasets->keras-cv) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras-cv) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras-core->keras-cv) (2.19.2)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow-datasets->keras-cv) (0.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras-cv) (1.70.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras-cv) (0.1.2)\n",
            "Downloading keras_cv-0.9.0-py3-none-any.whl (650 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m950.8/950.8 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras-core, keras-cv\n",
            "Successfully installed keras-core-0.1.7 keras-cv-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CO6_Ft_8T56A"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "from tensorflow.keras import models as tfkm\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#extra imports\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "from PIL import Image\n",
        "\n",
        "from keras_cv.layers import RandAugment\n",
        "\n",
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Functions\n"
      ],
      "metadata": {
        "id": "ZX12FgC60fxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess for ResNet"
      ],
      "metadata": {
        "id": "1qUsL1g9yKYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "\n",
        "def preprocess_images_in_batches_resnet(images, batch_size=1024):\n",
        "    \"\"\"\n",
        "    Preprocess images for ResNet in batches.\n",
        "\n",
        "    Parameters:\n",
        "    - images: NumPy array of shape (num_images, height, width, channels).\n",
        "    - batch_size: Number of images to process in each batch.\n",
        "\n",
        "    Returns:\n",
        "    - preprocessed_images: NumPy array with the same shape as images.\n",
        "    \"\"\"\n",
        "    # Placeholder for the preprocessed dataset\n",
        "    preprocessed_images = np.empty_like(images, dtype=np.float32)\n",
        "\n",
        "    # Calculate number of batches\n",
        "    num_batches = (len(images) + batch_size - 1) // batch_size\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min(start + batch_size, len(images))\n",
        "\n",
        "        # Preprocess the current batch\n",
        "        batch = images[start:end].astype('float32')  # Ensure float32 for preprocessing\n",
        "        preprocessed_images[start:end] = preprocess_input(batch)\n",
        "\n",
        "        # Free up memory by deleting the batch (not strictly necessary in Python)\n",
        "        del batch\n",
        "\n",
        "    return preprocessed_images"
      ],
      "metadata": {
        "id": "-_YkAqU6yOLo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add flipped version of minority class"
      ],
      "metadata": {
        "id": "bPm7SxpFAPr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flipped_minority_class(images, labels):\n",
        "    # Initialize lists to hold the flipped images and labels\n",
        "    flipped_images = []\n",
        "    flipped_labels = []\n",
        "    for i, img in enumerate(images):\n",
        "        # Flip the image horizontally\n",
        "        if labels[i] == 1:\n",
        "          flipped_img = np.fliplr(img)\n",
        "          flipped_images.append(flipped_img)\n",
        "          flipped_labels.append(labels[i])\n",
        "      # Append the flipped image and label to the lists\n",
        "    flipped_images = np.array(flipped_images)\n",
        "    flipped_labels = np.array(flipped_labels)\n",
        "    return flipped_images, flipped_labels"
      ],
      "metadata": {
        "id": "GhEGLG0DATjb"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXC6ao_arr7e"
      },
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print data info"
      ],
      "metadata": {
        "id": "txervoBlLzmd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_VssE_PRrxdL"
      },
      "outputs": [],
      "source": [
        "def print_data_info(data):\n",
        "  print(\"Keys in the .npz file:\", data.files)\n",
        "  for key in data.files:\n",
        "    print(f\"Array '{key}' - shape: {data[key].shape}, dtype: {data[key].dtype}\")\n",
        "\n",
        "  print(\"Number of label types, and count of elements in each label type\")\n",
        "  unique_labels, label_counts = np.unique(data['labels'], return_counts=True)\n",
        "  print(dict(zip(unique_labels, label_counts)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot bar charts"
      ],
      "metadata": {
        "id": "of6dN-GML3qT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tlVh1CcOsZ4x"
      },
      "outputs": [],
      "source": [
        "def plot_bar_chart(labels, label_counts):\n",
        "  plt.bar(labels, label_counts)\n",
        "  plt.xlabel('Label')\n",
        "  plt.ylabel('Count')\n",
        "  plt.title('Label Distribution')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjswoNferszX"
      },
      "source": [
        "Noting that the data is unbalanced, possible solutions; image augmentation, rotating every image pi/2, pi, 3/2*pi and adding weights to the features in the training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Display a random subsample of images\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ty3Nld2bL7RH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_samples(images, labels, num_samples=5):\n",
        "    unique_labels = np.unique(labels)\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        idx = np.where(labels == label)[0]\n",
        "        selected_images = np.random.choice(idx, num_samples, replace=False)\n",
        "        for j, img_idx in enumerate(selected_images):\n",
        "            plt.subplot(len(unique_labels), num_samples, i * num_samples + j + 1)\n",
        "            plt.imshow(images[img_idx])\n",
        "            plt.axis('off')\n",
        "            plt.title(f\"Class {label}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qMk0QIono8Qj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "source: https://machinelearningmastery.com/best-practices-for-preparing-and-augmenting-image-data-for-convolutional-neural-networks/"
      ],
      "metadata": {
        "id": "fuHNdaPE6ARS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print random pixel values"
      ],
      "metadata": {
        "id": "FTBbCuK1NjCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def print_pixel_values(images, num_images=5):\n",
        "    \"\"\"\n",
        "    Prints pixel values for a specified number of random images in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "        images (np.array): Array of images, assumed to be in the shape (num_images, height, width, channels).\n",
        "        num_images (int): Number of random images to inspect.\n",
        "    \"\"\"\n",
        "    # Choose random indices to select images from the dataset\n",
        "    indices = random.sample(range(images.shape[0]), num_images)\n",
        "\n",
        "    # Loop through selected indices and print pixel values for each image\n",
        "    for i, idx in enumerate(indices):\n",
        "        print(f\"\\nImage {i + 1} (Index {idx}):\")\n",
        "        print(\"Pixel values (sample):\")\n",
        "        print(images[idx, :5, :5, :])  # Print a 5x5 patch of pixels for brevity\n",
        "        print(f\"Min pixel value: {images[idx].min()}\")\n",
        "        print(f\"Max pixel value: {images[idx].max()}\\n\")"
      ],
      "metadata": {
        "id": "9dXdAHyGNlZ0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSliIxBvbs2Q"
      },
      "source": [
        "# üõ†Ô∏è Train and Save the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre processing"
      ],
      "metadata": {
        "id": "1ZX6J0_GJ57z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting data"
      ],
      "metadata": {
        "id": "W_DrySCtwZqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/X_train_MAIN.npy'\n",
        "train_labels_path = '/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/y_train_MAIN.npy'\n",
        "val_data_path = '/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/X_val_MAIN.npy'\n",
        "val_labels_path = '/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/y_val_MAIN.npy'\n",
        "\n",
        "# Load the datasets\n",
        "X_val = np.load(val_data_path)\n",
        "y_val = np.load(val_labels_path)\n",
        "X_train = np.load(train_data_path)\n",
        "y_train = np.load(train_labels_path)\n",
        "\n",
        "\n",
        "# Verify loaded data\n",
        "print(\"Datasets loaded successfully!\")\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}\")\n",
        "print(f\"y_val shape: {y_val.shape}\")"
      ],
      "metadata": {
        "id": "E36Pe59_vNvi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344
        },
        "outputId": "1ddc5aa8-2b8c-4723-a83c-22f0e7855d89"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/X_val_MAIN.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-201640050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gdrive/My Drive/[2024-2025] AN2DL/Homework 1/X_val_MAIN.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print number of elements in each class\n",
        "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(unique_labels, label_counts)))"
      ],
      "metadata": {
        "id": "UpH4Xl4j0Gat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding flipped versions of minority"
      ],
      "metadata": {
        "id": "WWp4JpNPAtA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flipped_images, flipped_labels = get_flipped_minority_class(X_train, y_train)\n",
        "\n",
        "print(flipped_labels[:2])\n",
        "\n",
        "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(unique_labels, label_counts)))\n",
        "\n",
        "X_train = np.concatenate((X_train, flipped_images))\n",
        "y_train = np.concatenate((y_train, flipped_labels))\n",
        "del flipped_images, flipped_labels\n",
        "\n",
        "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(unique_labels, label_counts)))"
      ],
      "metadata": {
        "id": "L50jbN6UAvv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get class weights"
      ],
      "metadata": {
        "id": "gGivc7mxWqxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of samples per class\n",
        "class_counts = {}\n",
        "class_counts[0] = np.sum(y_train == 0)\n",
        "class_counts[1] = np.sum(y_train == 1)\n",
        "\n",
        "print(f\"class counts: {class_counts}\")\n",
        "\n",
        "# Get the unique classes and their counts\n",
        "classes = np.array(list(class_counts.keys()))\n",
        "counts = np.array(list(class_counts.values()))\n",
        "\n",
        "# Calculate class weights\n",
        "total_samples = sum(counts)\n",
        "class_weights = {i: total_samples / (len(class_counts) * count) for i, count in class_counts.items()}\n",
        "print(f\"class weights: {class_weights}\")"
      ],
      "metadata": {
        "id": "OUVMFZKKWtLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocess"
      ],
      "metadata": {
        "id": "fyO1Z2tiWvXU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "print(f\"X_train shape before preprocess: {X_train.shape}\")\n",
        "print(f\"y_train shape before preprocess: {y_train.shape}\")\n",
        "\n",
        "\n",
        "#print number of elements in each class\n",
        "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
        "print(dict(zip(unique_labels, label_counts)))\n",
        "\n",
        "X_train = preprocess_images_in_batches_resnet(X_train)\n",
        "X_val = preprocess_images_in_batches_resnet(X_val)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "\n",
        "print(f\"X_train shape after preprocess: {X_train.shape}\")\n",
        "print(f\"y_train shape after preprocess: {y_train.shape}\")\n",
        "\n",
        "#print number of elements in each class\n",
        "class_counts = np.sum(y_train, axis=0)\n",
        "print(f\"class counts: {dict(enumerate(class_counts))}\")"
      ],
      "metadata": {
        "id": "g-SbaWFsxUIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting variables"
      ],
      "metadata": {
        "id": "uGTKQ2WDKqpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Uses a lot of memory to run\n",
        "HEIGHT = X_train[0].shape[0]\n",
        "WIDTH = X_train[0].shape[1]\n",
        "CHANNELS = X_train[0].shape[2]\n",
        "NUM_CLASSES = len(np.unique(y_train))\n",
        "\n",
        "\n",
        "# Number of training epochs\n",
        "epochs = 15\n",
        "\n",
        "# Batch size for training\n",
        "batch_size = 64\n",
        "\n",
        "# Learning rate: step size for updating the model's weights\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Print the defined parameters\n",
        "print(\"Epochs:\", epochs)\n",
        "print(\"Batch Size:\", batch_size)\n",
        "print(\"Learning Rare:\", learning_rate)\n",
        "print(\"Image height:\", HEIGHT)\n",
        "print(\"Image width:\", WIDTH)\n",
        "print(\"Image channels:\", CHANNELS)\n",
        "print(\"Number of classes:\", NUM_CLASSES)"
      ],
      "metadata": {
        "id": "VbC_vKFiKtz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining *weights*"
      ],
      "metadata": {
        "id": "rdIzn4mkOXDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model definition"
      ],
      "metadata": {
        "id": "YrnxVV2mOlM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "def build_model(\n",
        "    input_shape=(HEIGHT, WIDTH, CHANNELS),\n",
        "    output_shape=NUM_CLASSES,\n",
        "    learning_rate=learning_rate,\n",
        "    seed=seed\n",
        "):\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "    # Load ResNet-50 as the base model\n",
        "    base_model = ResNet50(\n",
        "        input_shape=input_shape,\n",
        "        include_top=False,  # Exclude the classification head\n",
        "        weights='imagenet'  # Load pre-trained ImageNet weights\n",
        "    )\n",
        "\n",
        "    # Freeze the base model to prevent training its pre-trained weights\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Add custom classification head\n",
        "    inputs = tfkl.Input(shape=input_shape, name='Input')\n",
        "    x = base_model(inputs, training=False)  # Pass input through base model\n",
        "    x = tfkl.GlobalAveragePooling2D(name='global_avg_pool')(x)  # Pool features\n",
        "    x = tfkl.Dense(128, activation='relu', name='dense_1')(x)   # First dense layer\n",
        "    x = tfkl.Dropout(0.3, seed=seed, name='dropout_1')(x)       # Dropout for regularization\n",
        "    x = tfkl.Dense(64, activation='relu', name='dense_2')(x)    # Second dense layer\n",
        "    outputs = tfkl.Dense(units=output_shape, activation='softmax', name='output')(x)\n",
        "\n",
        "    # Create the final model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='class_1_detector')\n",
        "\n",
        "    # Compile the model\n",
        "    loss = tfk.losses.CategoricalCrossentropy()  # Adjust loss function for one-hot labels\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    metrics = ['accuracy', 'precision', 'recall']\n",
        "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "TJwf1OohzOZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model"
      ],
      "metadata": {
        "id": "3qCMPDQPzlhM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with specified input and output shapes\n",
        "model = build_model()\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "#tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "nAr6nsGizkrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "Dj2W3JsFzsef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the patience value for early stopping\n",
        "patience = 10\n",
        "\n",
        "# Create an EarlyStopping callback\n",
        "early_stopping = tfk.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    patience=patience,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,  # Reduce learning rate by half\n",
        "    patience=3,  # After 3 epochs of no improvement\n",
        "    min_lr=1e-6  # Set a minimum learning rate\n",
        ")\n",
        "\n",
        "# Store the callback in a list\n",
        "callbacks = [early_stopping, reduce_lr]"
      ],
      "metadata": {
        "id": "JbxboVMMzyh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with early stopping callback\n",
        "history = model.fit(\n",
        "    x=X_train,\n",
        "    y=y_train,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(X_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights\n",
        ").history\n",
        "\n",
        "# Calculate and print the final validation accuracy\n",
        "final_val_accuracy = round(max(history['val_accuracy'])* 100, 2)\n",
        "print(f'Final validation accuracy: {final_val_accuracy}%')\n",
        "\n",
        "# Save the trained model to a file with the accuracy included in the filename\n",
        "model_filename = 'class7.keras'\n",
        "model.save(model_filename)\n",
        "\n",
        "# Delete the model to free up resources\n",
        "del model"
      ],
      "metadata": {
        "id": "OXXTJ5wKz238"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_val, y_val_pred_classes)\n",
        "precision = precision_score(y_val, y_val_pred_classes, average='weighted')  # Use weighted average for multi-class\n",
        "recall = recall_score(y_val, y_val_pred_classes, average='weighted')\n",
        "f1 = f1_score(y_val, y_val_pred_classes, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(f'Validation Precision: {precision}')\n",
        "print(f'Validation Recall: {recall}')\n",
        "print(f'Validation F1-Score: {f1}')"
      ],
      "metadata": {
        "id": "M51R1z1xmNfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(15, 2))\n",
        "plt.plot(history['loss'], label='Training loss', alpha=.8)\n",
        "plt.plot(history['val_loss'], label='Validation loss', alpha=.8)\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "\n",
        "# Plot training and validation accuracy\n",
        "plt.figure(figsize=(15, 2))\n",
        "plt.plot(history['accuracy'], label='Training accuracy', alpha=.8)\n",
        "plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.8)\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(alpha=.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VjNMzJ2Cz8z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_val)\n",
        "y_pred_classes = (y_pred > 0.5).astype(int)\n",
        "print(classification_report(y_val, y_pred_classes, target_names=[\"negative\", \"positive\"]))\n"
      ],
      "metadata": {
        "id": "TySwUeF3KTh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Idk hva dette er"
      ],
      "metadata": {
        "id": "JnKIb_9b0LjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_activations(model, X, num_images):\n",
        "\n",
        "    # Identify the first convolutional layer\n",
        "    first_conv_index = None\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        if isinstance(layer, tfk.layers.Conv2D):\n",
        "            first_conv_index = i\n",
        "            break\n",
        "\n",
        "    if first_conv_index is None:\n",
        "        raise ValueError(\"The model does not contain a 2D convolution\")\n",
        "\n",
        "    # Extract activations from the first convolutional layer\n",
        "    first_conv = tfk.Sequential(model.layers[:first_conv_index + 1])\n",
        "    first_activations = first_conv(X[:num_images])\n",
        "\n",
        "    # Identify the first pooling layer after the first convolution\n",
        "    pooling_index = None\n",
        "    for i, layer in enumerate(model.layers[first_conv_index + 1:], start=first_conv_index + 1):\n",
        "        if isinstance(layer, (tfk.layers.MaxPooling2D, tfk.layers.AveragePooling2D)):\n",
        "            pooling_index = i\n",
        "            break\n",
        "\n",
        "    if pooling_index is None:\n",
        "        raise ValueError(\"The model does not contain a 2D pooling operation after the first convolution\")\n",
        "\n",
        "    # Extract activations from the first convolution and the first pooling layer\n",
        "    second_conv = tfk.Sequential(model.layers[:pooling_index + 1])\n",
        "    second_activations = second_conv(X[:num_images])\n",
        "\n",
        "    return first_activations, second_activations\n",
        "\n",
        "def find_last_conv_layer(model):\n",
        "\n",
        "    # Identify the last convolutional layer in the model\n",
        "    for layer in reversed(model.layers):\n",
        "        if isinstance(layer, tfk.layers.Conv2D):\n",
        "            return layer.name\n",
        "    raise ValueError(\"No Conv2D layer found in the model.\")\n",
        "\n",
        "def visualize(model, X, y, unique_labels, num_images=50, display_activations=True):\n",
        "\n",
        "    # Extract activations from the model\n",
        "    first_activations, second_activations = extract_activations(model, X, num_images)\n",
        "\n",
        "    # Select a random image for prediction and visualisation\n",
        "    image = np.random.randint(0, num_images)\n",
        "    predictions = model.predict(np.expand_dims(X[image], axis=0), verbose=0)\n",
        "    class_int = np.argmax(predictions[0])\n",
        "    class_str = unique_labels[class_int]\n",
        "\n",
        "    # Create figure layout for displaying the image and predictions\n",
        "    fig = plt.figure(constrained_layout=True, figsize=(16, 4))\n",
        "    gs = gridspec.GridSpec(1, 2, figure=fig, width_ratios=[1.5, 1.5], wspace=0)\n",
        "\n",
        "    # Display the selected image with the true class\n",
        "    ax1 = fig.add_subplot(gs[0])\n",
        "    ax1.set_title(f\"True class: {unique_labels[np.argmax(y[image])]}\", loc='left')\n",
        "    if X[image].shape[-1] == 1:\n",
        "        ax1.imshow(np.squeeze(X[image]), cmap='bone', vmin=0., vmax=1.)\n",
        "    else:\n",
        "        ax1.imshow(np.squeeze(X[image]), vmin=0., vmax=1.)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    # Display the prediction bar\n",
        "    ax2 = fig.add_subplot(gs[1])\n",
        "    ax2.barh(unique_labels, np.squeeze(predictions, axis=0), color=plt.get_cmap('tab10').colors)\n",
        "    ax2.set_title(f\"Predicted class: {class_str} (Confidence: {max(np.squeeze(predictions, axis=0)):.2f})\", loc='left')\n",
        "    ax2.grid(alpha=0.3)\n",
        "    ax2.set_xlim(0.0, 1.0)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Display activations if required\n",
        "    if display_activations:\n",
        "        # Visualise the activations from the first convolutional layer\n",
        "        fig, axes = plt.subplots(1, 8, figsize=(16, 14))\n",
        "        for i in range(8):\n",
        "            ax = axes[i]\n",
        "            ax.imshow(first_activations[image, :, :, i], cmap='bone', vmin=0., vmax=1.)\n",
        "            ax.axis('off')\n",
        "            if i == 0:\n",
        "                ax.set_title('First convolution activations', loc='left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Visualise the activations from the first pooling layer\n",
        "        fig, axes = plt.subplots(2, 8, figsize=(16, 5))\n",
        "        for i in range(16):\n",
        "            ax = axes[i // 8, i % 8]\n",
        "            ax.imshow(second_activations[image, :, :, i], cmap='bone', vmin=0., vmax=1.)\n",
        "            ax.axis('off')\n",
        "            if i == 0:\n",
        "                ax.set_title('Second convolution activations', loc='left')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Example usage\n",
        "visualize(model, X_test, y_test, unique_labels, display_activations=True)"
      ],
      "metadata": {
        "id": "1fRRyXSG0N7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels for the entire test set\n",
        "predictions = model.predict(X_test, verbose=0)\n",
        "\n",
        "# Display the shape of the predictions\n",
        "print(\"Predictions Shape:\", predictions.shape)"
      ],
      "metadata": {
        "id": "5KFq2XOY0RRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = tfk.models.load_model('CIFAR10_CNN_69.92.keras')\n",
        "\n",
        "# Display a summary of the model architecture\n",
        "model.summary(expand_nested=True, show_trainable=True)\n",
        "\n",
        "# Plot the model architecture\n",
        "tfk.utils.plot_model(model, expand_nested=True, show_trainable=True, show_shapes=True, dpi=70)"
      ],
      "metadata": {
        "id": "erNOcJMT0Bfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Old Model"
      ],
      "metadata": {
        "id": "0QzchF6Zz45V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox9jqYyyUJo0"
      },
      "outputs": [],
      "source": [
        "model = tfkm.Sequential([\n",
        "    tfkl.InputLayer(input_shape=(HEIGHT, WIDTH, CHANNELS)),  # Input shape\n",
        "    tfkl.Conv2D(32, (3, 3), activation='relu'),  # First convolutional layer\n",
        "    tfkl.MaxPooling2D((2, 2)),  # Pooling layer\n",
        "\n",
        "    tfkl.Conv2D(64, (3, 3), activation='relu'),  # Second convolutional layer\n",
        "    tfkl.MaxPooling2D((2, 2)),  # Pooling layer\n",
        "\n",
        "    tfkl.Conv2D(128, (3, 3), activation='relu'),  # Third convolutional layer\n",
        "    tfkl.MaxPooling2D((2, 2)),  # Pooling layer\n",
        "\n",
        "    tfkl.Flatten(),  # Flatten the output from convolutional tfkl\n",
        "    tfkl.Dense(128, activation='relu'),  # Fully connected layer\n",
        "\n",
        "    tfkl.Dense(NUM_CLASSES, activation='softmax'),  # Output layer (softmax for multi-class classification)\n",
        "  ])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_dataset, epochs=10)\n",
        "\n",
        "model.save('weights.keras')\n",
        "\n",
        "#testing model on test-set\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "\n",
        "test_dataset = test_dataset.batch(batch_size)\n",
        "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n",
        "\n",
        "del model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNp6pUZuddqC"
      },
      "source": [
        "# üìä Prepare Your Submission\n",
        "\n",
        "To prepare your submission, create a `.zip` file that includes all the necessary code to run your model. It **must** include a `model.py` file with the following class:\n",
        "\n",
        "```python\n",
        "# file: model.py\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the internal state of the model.\"\"\"\n",
        "        self.neural_network = tf.keras.models.load_model('weights.keras')\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Return a numpy array with the labels corresponding to the input X.\"\"\"\n",
        "```\n",
        "\n",
        "The next cell shows an example implementation of the `model.py` file, which includes loading model weights from the `weights.keras` file and conducting predictions on provided input data. The `.zip` file is created and downloaded in the last notebook cell.\n",
        "\n",
        "‚ùó Feel free to modify the method implementations to better fit your specific requirements, but please ensure that the class name and method interfaces remain unchanged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKT4h-9xYwiT"
      },
      "outputs": [],
      "source": [
        "%%writefile model.py\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "import cv2\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize the internal state of the model. Note that the __init__\n",
        "        method cannot accept any arguments.\n",
        "\n",
        "        The following is an example loading the weights of a pre-trained\n",
        "        model.\n",
        "        \"\"\"\n",
        "        self.neural_network = tfk.models.load_model('weights.keras')\n",
        "\n",
        "\n",
        "    def make_background_white(self, image, lower_purple=(105, 25, 25), upper_purple=(165, 255, 255),\n",
        "                              lower_red=(165, 70, 130), upper_red=(210,150,150)):\n",
        "        \"\"\"\n",
        "        Process an image to replace its background with white if it contains colors within the specified purple or red ranges.\n",
        "        \"\"\"\n",
        "        # Ensure image is numpy array with correct type\n",
        "        if isinstance(image, tf.Tensor):\n",
        "            image = image.numpy()\n",
        "        assert isinstance(image, np.ndarray), f\"Expected numpy array, got {type(image)}\"\n",
        "        assert image.dtype == np.uint8, f\"Expected uint8 data type, got {image.dtype}\"\n",
        "\n",
        "        print(f\"Image dtype: {image.dtype}, Image shape: {image.shape}\")\n",
        "\n",
        "        # Convert RGB to HSV\n",
        "        hsv_image = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2HSV)\n",
        "\n",
        "        # Create masks\n",
        "        purple_mask = cv2.inRange(hsv_image, lower_purple, upper_purple)\n",
        "        red_mask = cv2.inRange(hsv_image, lower_red, upper_red)\n",
        "        combined_mask = cv2.bitwise_or(purple_mask, red_mask)\n",
        "        background_mask = cv2.bitwise_not(combined_mask)\n",
        "\n",
        "        white_background = np.full_like(image, 255)\n",
        "        result_image = cv2.bitwise_and(image, image, mask=combined_mask)\n",
        "        result_image = cv2.add(result_image, cv2.bitwise_and(white_background, white_background, mask=background_mask))\n",
        "\n",
        "        return result_image\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the labels corresponding to the input X. Note that X is a numpy\n",
        "        array of shape (n_samples, 96, 96, 3) and the output should be a numpy\n",
        "        array of shape (n_samples,). Therefore, outputs must no be one-hot\n",
        "        encoded.\n",
        "\n",
        "        The following is an example of a prediction from the pre-trained model\n",
        "        loaded in the __init__ method.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Making background white\n",
        "        X = np.array([self.make_background_white(image) for image in X])\n",
        "        X = X / 255.0\n",
        "\n",
        "\n",
        "        # Cropping to correct size\n",
        "        # NEW_SIZE = (96, 96)\n",
        "        # X = tf.image.resize_with_crop_or_pad(X, NEW_SIZE[0], NEW_SIZE[1])\n",
        "\n",
        "        #Converting back to tensor (temp solution)\n",
        "        # X = tf.convert_to_tensor(X, dtype=tf.uint8)\n",
        "\n",
        "        # Normalizing the images\n",
        "        # X = tf.cast(X, tf.float32) / 255.0\n",
        "\n",
        "        preds = self.neural_network.predict(X)\n",
        "        if len(preds.shape) == 2:\n",
        "            preds = np.argmax(preds, axis=1)\n",
        "        return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s18kX1uDconq"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "filename = f'submission_{datetime.now().strftime(\"%y%m%d_%H%M%S\")}.zip'\n",
        "\n",
        "# Add files to the zip command if needed\n",
        "!zip {filename} model.py weights.keras\n",
        "\n",
        "from google.colab import files\n",
        "files.download(filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "txervoBlLzmd",
        "of6dN-GML3qT",
        "FTBbCuK1NjCR",
        "WWp4JpNPAtA_",
        "JnKIb_9b0LjV",
        "0QzchF6Zz45V"
      ],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}